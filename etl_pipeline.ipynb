{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxfgwdsg4rrxdI4J0sDxRZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjeevmanvithvellala/DATA-PIPELINE-DEVELOPMENT/blob/main/etl_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os"
      ],
      "metadata": {
        "id": "kRthsUDOaiXn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 1. Extract ==========\n",
        "def extract_data(file_path):\n",
        "    print(\"[Extract] Loading data from:\", file_path)\n",
        "    return pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "uGhFz78raidi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 2. Transform ==========\n",
        "def transform_data(df):\n",
        "    print(\"[Transform] Starting data transformation...\")\n",
        "\n",
        "    # Separate features and target (assuming last column is the target)\n",
        "    X = df.iloc[:, :-1]\n",
        "    y = df.iloc[:, -1]\n",
        "\n",
        "    # Identify column types\n",
        "    numerical_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "    categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n",
        "\n",
        "    # Pipelines for transformation\n",
        "    numeric_pipeline = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='mean')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    categorical_pipeline = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    # Combine both pipelines\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_pipeline, numerical_cols),\n",
        "            ('cat', categorical_pipeline, categorical_cols)\n",
        "        ])\n",
        "\n",
        "    X_processed = preprocessor.fit_transform(X)\n",
        "    print(\"[Transform] Transformation complete.\")\n",
        "\n",
        "    return X_processed, y"
      ],
      "metadata": {
        "id": "9yTM7_A5aimH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 3. Load ==========\n",
        "def load_data(X, y, output_path='processed_data.csv'):\n",
        "    print(\"[Load] Saving transformed data...\")\n",
        "    # If X is a NumPy array, convert to DataFrame\n",
        "    if not isinstance(X, pd.DataFrame):\n",
        "        X = pd.DataFrame(X.toarray() if hasattr(X, \"toarray\") else X)\n",
        "\n",
        "    df_out = pd.concat([X, y.reset_index(drop=True)], axis=1)\n",
        "    df_out.to_csv(output_path, index=False)\n",
        "    print(\"[Load] Data saved to:\", output_path)"
      ],
      "metadata": {
        "id": "sU4ZNrXsaiuZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== Main Execution ==========\n",
        "def run_etl_pipeline(input_file):\n",
        "    df = extract_data(input_file)\n",
        "    X_processed, y = transform_data(df)\n",
        "    load_data(X_processed, y)"
      ],
      "metadata": {
        "id": "ufJZACXhazRg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== Example Usage ==========\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace with your file path\n",
        "    input_csv = \"sample_data.csv\"\n",
        "\n",
        "    if os.path.exists(input_csv):\n",
        "        run_etl_pipeline(input_csv)\n",
        "    else:\n",
        "        print(\"❌ File not found. Please check the path.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hf9eE3jazV7",
        "outputId": "942cc551-c02e-4042-e985-0c00cff8dc85"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ File not found. Please check the path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing With the Sample_data.csv provided**"
      ],
      "metadata": {
        "id": "L33gq9bUcJv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 1. Extract ==========\n",
        "def extract_data(file_path):\n",
        "    print(\"[Extract] Loading data from:\", file_path)\n",
        "    return pd.read_csv(file_path)\n"
      ],
      "metadata": {
        "id": "kZneDb3jbkri"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 2. Transform ==========\n",
        "def transform_data(df, target_column=None):\n",
        "    print(\"[Transform] Starting data transformation...\")\n",
        "\n",
        "    if target_column:\n",
        "        y = df[target_column]\n",
        "        X = df.drop(columns=[target_column])\n",
        "    else:\n",
        "        X = df.iloc[:, :-1]\n",
        "        y = df.iloc[:, -1]\n",
        "\n",
        "    # Identify column types\n",
        "    numerical_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "    categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n",
        "\n",
        "    # Pipelines for transformation\n",
        "    numeric_pipeline = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='mean')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    categorical_pipeline = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    # Combine both pipelines\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_pipeline, numerical_cols),\n",
        "            ('cat', categorical_pipeline, categorical_cols)\n",
        "        ])\n",
        "\n",
        "    X_processed = preprocessor.fit_transform(X)\n",
        "    print(\"[Transform] Transformation complete.\")\n",
        "\n",
        "    # Output feature names (optional debug)\n",
        "    feature_names = preprocessor.get_feature_names_out()\n",
        "    print(\"[Transform] Transformed feature names:\")\n",
        "    print(feature_names)\n",
        "\n",
        "    return X_processed, y"
      ],
      "metadata": {
        "id": "g28dERdIeCYT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 3. Load ==========\n",
        "def load_data(X, y, output_path='processed_data.csv'):\n",
        "    print(\"[Load] Saving transformed data...\")\n",
        "    if not isinstance(X, pd.DataFrame):\n",
        "        X = pd.DataFrame(X.toarray() if hasattr(X, \"toarray\") else X)\n",
        "\n",
        "    df_out = pd.concat([X, y.reset_index(drop=True)], axis=1)\n",
        "    df_out.to_csv(output_path, index=False)\n",
        "    print(\"[Load] Data saved to:\", output_path)"
      ],
      "metadata": {
        "id": "xXVKwhoyeCb9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== Main Execution ==========\n",
        "def run_etl_pipeline(input_file, target_column=\"Churn\"):\n",
        "    df = extract_data(input_file)\n",
        "    X_processed, y = transform_data(df, target_column)\n",
        "    load_data(X_processed, y)"
      ],
      "metadata": {
        "id": "vmQmS5K3eCfY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== Example Usage ==========\n",
        "if __name__ == \"__main__\":\n",
        "    input_csv = \"sample_data.csv\"\n",
        "\n",
        "    if os.path.exists(input_csv):\n",
        "        run_etl_pipeline(input_csv)\n",
        "    else:\n",
        "        print(\"❌ File not found. Please check the path.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zhm9e4VeCp6",
        "outputId": "8d24d82e-1697-40e1-8655-d01e4826d4a1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Extract] Loading data from: sample_data.csv\n",
            "[Transform] Starting data transformation...\n",
            "[Transform] Transformation complete.\n",
            "[Transform] Transformed feature names:\n",
            "['num__Age' 'num__Income' 'cat__Name_Alice' 'cat__Name_Bob'\n",
            " 'cat__Name_Charlie' 'cat__Name_Diana' 'cat__Name_Eva' 'cat__Name_Frank'\n",
            " 'cat__Name_Grace' 'cat__Name_Henry' 'cat__Name_Isla' 'cat__Name_Jake'\n",
            " 'cat__Gender_Female' 'cat__Gender_Male' 'cat__City_Chicago'\n",
            " 'cat__City_Los Angeles' 'cat__City_New York' 'cat__City_San Francisco']\n",
            "[Load] Saving transformed data...\n",
            "[Load] Data saved to: processed_data.csv\n"
          ]
        }
      ]
    }
  ]
}